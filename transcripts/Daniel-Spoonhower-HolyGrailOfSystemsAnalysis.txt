      "The Holy Grail of Systems Analysis: from What to Where to Why."
      By: Daniel Spoonhower.
      >> Great.  Thanks, everybody, for coming.  My name is Spoons.  My parents still call me Dan but nobody that I hang out with or work with does, really.  I'm a engineer, I love to geek out about programming languages and garbage collection.  But today I'm going to talk about running processing systems, which is something I'm excited about.  Where I work now, we built a tool for doing distributed monitoring, so I spent a lot of time thinking about this.  The tool itself is also a distributed system, so I also care about it from that point of view.  And I also participate in a thing called open tracing that I'll mention a couple of times during the talk.
      So what I'm going to talk about, I think is sort of the essence of monitoring.  I guess if I follow from this morning, observability, I'm sure I can keep the two straight.  But really trying to go, understand for our software systems, what's going wrong, where's the problem?  And where is that happening?
      And I think anyone who is responsible for production systems, cares about these questions, and they want to find answers to them quickly and with confidence.  And with -- as little investment as possible.  And I think there's a real opportunity for us to do better in terms of the tooling we have.  But it requires us to think a little bit more broadly about first kind of the existing tools and the way we've kind of maybe segmented them.  And then secondly, about the way we collect and analyze the day that we have.
      And so what I kind of hope that you all take with you today is just maybe some questions about how we're doing that and I'm going to show some examples of how I think we can do better in the future.
      I think we're here because we like building systems, and we like helping others to build systems.  That's why I'm here.  That's hard, building systems is hard.  If it was easy, we probably wouldn't be here.  And I think one of the big challenges in running distributed systems was mediating problems when something's gone wrong.  Downtime, broken deployments, those can mean a loss of revenue, loss of users, they can damage your brand.  So the faster can fix a problem, the faster your users can get back to whatever they were doing, and you can get back to sleep.
      If you went to the talk yesterday, he said a lot of things about how things can go wrong and how you can understand those vulnerabilities and adjust them before they become outages.  But, of course, some things will go wrong in production.
      So I think the way I think about it, the purpose of monitoring is to tell stories.  I don't think anyone's going to argue with that.  Probably some argument about the way that monitoring tools do that.  Maybe that's why some people came this weekend to argue about that.
      But I think if a monitoring tool doesn't tell a clear story, it's not really working.  And ideally, that story takes you to why the problem is happening, and it takes you to a solution to that problem.  And there's a number of factors that have led to our tools becoming less effective, recently.  Some of these might sound a little bit familiar.
      But I think for monitoring, it means something a little bit different, maybe.  Microservices are here.  I don't think anyone's arguing with that anymore.  It's great.  I'm excited about them.  There's a lot of cool things about microservices.
      But they broke our tools.  In that each transaction is still a single story.  But now not just one story material but a whole set of them.
      And I think this is kind of a big deal.  Bigger than the move to VMs or the move to the cloud.  I think about a lot more, like, the transition between desktop software where we took something out of shrink wrap and installed a CD or DVD or floppies or something and transitioning to a client server architecture.  The tools that we had for desktop software just didn't work for client software.  I think the shift of microservices are a categorical shift, and I think there's a similar categorical breakage in the tools that we're using and the workflows that we have.
      So back in the day, applications look like this.  They were extremely symmetric, they were square.  They were one square.  And maybe that box is broken into a bunch of different libraries or packages.  But it all fit together in a single process.
      And.  Okay.  Maybe it wasn't one process.  Maybe it was, like, three.  But I guess the idea is everyone in your org probably knew all three boxes, they knew what they did and how they interacted.  But the deal with microservices is that isn't true anymore; right?  Like, I don't even know all the names of the services.  There's too many to keep track of.  They're changing all the time.  That's great.  Now that those things can be deployed independently and scaled independently, not so great that they're being monitored independently.
      So it used to be that tracing kind of old school used to mean just follow the path of execution through a single process, and that was pretty easy.  Just walked up a stack.  That's why I have print stack trace.  But when you're doing that in a distributed system, it means that you really need to follow that path across process boundaries.  And that's not as easy.  But really, doing that is kind of table stakes for monitoring distributed systems.  If you're not doing that, it's going to be really hard to understand what's happening.  I think if you see folks moving toward microservices, even just starting down that path.  Like, I'm not talking Netflix stile things just ten services.  And if you don't have some solution, it's going to be pretty painful.  And, again, not even, like, complicated things like P99.9 latency or something like that.  Just something's on fire, what's going on?  I think tracing is really important for that kind of thing.
      There's another problem that has come up.  Maybe not quite so recently.  But, you know, there's a lot of things happening, and I would like to imagine in my simple, single core brain that requests are atomic and serialized, but they're not.  Of course, there's a lot of performance and other good reasons, a lot of things happening at the same time.  And that's not quite true either.  There's a lot of asynchronous things happening, you know, requests get interrupted, they perform IO.  Maybe you're using some framework like node or go routines.  There's a lot of stuff happening, process a request will start, it will pause for some reason, other things will run for a while, and it will come back and so on.
      In a distributed system, you've got all of that, and it's worse.  You've got all of that in every process, and now you've got single transactions, and I think multiple processes.
      So and even that picture itself is kind of an oversimplification because everything, if you look kind of closely is still linear thing.  Generally, the whole part of the reason you're doing this is so that you can fork off the requests, the processing in parallel and then joining the results back together as you go back up the stack.  I sort of ran out of room on this slide to do that, though.
      Anyway, the goal is to, like, connect all of that stuff back and then isolate one of those requests so that you can actually understand what's happening and then try to address the problem.
      So, yeah, and, you know, we can do that.  But I think I'll have a chance to say a little bit more later about maybe why that isn't quite enough.
      So anyway, that's sort of where we are.  I think those problems are not going way away, so now the question is how we're going to deal with them.  And that's where tools come in.  And I think like I said tools are going to kind of address these three questions, and that's how I'm going to structure the rest of the talk.
      Okay.  So what's going on?
      Answering this question is actually not so bad today.  We've got some pretty good tools for doing this.  So we can use metrics tools for measuring symptoms and symptoms are a pretty good way of modeling what's going on.  So, you know, you can make a metric that associates with different end user experience that can be latency, it can be errors, a few other things, and he can you can set SLAs or goals around those metrics and set an alert or notification when you miss your goal.
      And I should say I brought end user here and end user might be an actual human being.  If you're a platform, your user might be either an app or another application that's built on top of you.  But the point is that you can model pretty easily when your clients are seeing using metrics.
      So how do we explain anomalies with metrics?
      So, you know, we might have something that looks like this.  I just grabbed some rabid thing from one of our internal dashboards.  So I don't know if this was error rate or something like that.  But, you know, somethings up about 12:30.  Better figure what's going on.  Nothing was pushed out then, so let's dig a little bit deeper.  Luckily, we use tags as most metric systems support to tag things based upon customer, service, what VM it's running on, things like that.  And then we can group by the data to break that apart and drill down and see what's happening.
      So that's cool.  You can see, I think, that, you know, most of that bump actually can be attributed to this one component.  We can look at the tag that that thing represents and understand what's up with that customer or service or whatever.
      So that's great.  We've gotten somewhere.  But this kind of big problem, mentioned this morning.  We say cardinality sometimes.  I feel this is a pretty big ward to talk about a simple thing.  But some tags have a lot of different values.  And when that happens, it becomes really expensive to collect that data, really expensive to manage it.  And when I say a lot, I mean, more than 100, and it's not uncommon.  You have a customer ID, you have, obviously, hundreds or hundreds of thousands or millions of values for these tags.  So, yeah.  That can be a problem.  It means, like, maybe this isn't quite so useful.  But that's sort of one problem.  The second one I want to say is this is not a diagnosis.  Like, we have a smoking gun here.  So we can sort of narrow a little bit.  We don't know, you know, why that is customer is misbehaving, what that service is doing or what's wrong with that; right?
      So in some level, it's neither scalable nor actually delivering what we really want here.
      Another problem with metrics is you start to get dashboards that look a little bit like this.  And that can be a little bit overwhelming.  Having lots of metrics can seem great, but I think it's important not to confuse having lots of answers to the what question.  Not to confuse that with an answer to the question of why something has gone wrong.
      We used to have giant dashboards like this and the teams I worked on at Google, they were really cool with and they looked really impressive.  But the thing is unless you were an expert in the system that was being described, like, the dashboard was not that helpful; right?  So I don't know I just, you know, I'll admit I still use dashboards.  Not everybody's perfect.
      This is some random metrics that I grabbed from our service.  And, you know, we had an incident here.  It wasn't terrible, but it wasn't great.  Anyone can help me out with the root cause here, like, upper left, lower right, anything like that?
      Yeah, so you can see correlation pretty easy in something like this, but it's pretty hard to determine causation, and that's really what you want to do.  In distributed systems, you have lots of inner connections between these things, so it's pretty common for lots of metrics to pull up at the same time.
      Actually, I don't really remember what this is about either.  Like, this dashboard has changed a lot, these services changed a lot, and that's kind of -- the next problem, which is that, you know, the number of metrics, if you talk about your business in terms of what matters to your customers, like, that should stay relatively small.  You know, of course, with future changes, you might care about a few more.  But as you develop more and more microservices within your system, you'll be tempted to create more and more metrics.  And that's a problem for a bunch of reasons.  One, that's just a lot of maintenance.  Like, that dashboard is outdated, it's no good.  It can get costly from a storage point of view, even before we start talking about tags.  If the data you have is scaling both with the number of customers and the number of microservices or the number of transactions, like, that's not great for your business.  And -- I mean, this is only going to be helpful if you actually define metrics for all of the root causes, which you didn't; right?  There's some root causes that you haven't seen yet, so it's hard to anticipate what those are going to be, and you're not going to be able to find metrics.  So in the end, metrics aren't the answer to the root cause analysis.  They're useful, but they're not the end of the story.
      Okay.  So chapter two.  Where is the problem in my system?  And this and actually most of the talk, I'm going to talk a lot about latency, but this could apply to errors orer things you might measure as well.
      Okay.  So figuring out where our problem is.  This is sort of classic distributed tracing.  How many people here do distributed tracing?  Like doing distributed tracing?  Have heard of distributed tracing?  Okay.  Cool.  Awesome.  I guess that's why you're here, maybe.
      So at Google, we have this tool called dapper, and it was cool for this kind of thing.  You get paged, you have some message, user latency was up, it would tell you where the latency came from, and you do a critical path analysis and then you kind of go from there.
      So I want to do a little bit of illustration.  So I have kind of a side project with one of my cofounders we call doughnut zone, which is doughnuts as a service solution.  We're doing some blitz sale right now, we're microservice-oriented, so we're pretty ready to deal with that growth.  But we are having some latency problems, and we use open tracing, so it turns out we can great with a bunch of different vendors pretty easily.
      So let me just give you a picture of what our architecture look like.  There's three different clients, a client used to order doughnuts, we have restacking ingredients or cleaning out some of the equipment.  Those all talk to an API service.  API service then goes to some other back-end.  So we use brain pal for payments and then service that are doing a lot of the heavy lifting.
      Because we're real computer scientists and, you know, we want to do it right, we use around the resources that are here.  So we don't want to be cleaning the fire we're frying doughnuts.  We don't want to be restacking the shelves while you're grabbing the toppings.  It's going to make a mess.  And where we have mutexs.  You're going to have cues.  What I want you to think about is there's cues everywhere, whether that's a database table, a network link, a processor, these are all resources where you can have some contention.  This is tiny in some level, but I hope it's real enough that you can squint and suspend your since of grief enough.
      I'm a little hungry.  I'm going to order some doughnuts here.  This is part one of the demo.  You guys are going to have a chance to participate in a bit, but let me mess around a little bit here.
      Doughnut zone, let's see what we have going on for doughnuts.  We don't have an Internet connection.  This is going to be a little bit of a problem for my demo.
      Sorry.  I should have checked this.
      Give me one second.
      Okay.  So I'm going to order some doughnuts here.  All right.  Super.  Cool.
      All right.  What do I want?  Let's have some chocolate doughnuts here.  There's a little thing on here that shows you how it's doing.  And then I'm going to flip over here to my tracing system and check out how chocolate doughnuts are doing here.  So I've got some examples.  These ones took a couple of seconds.
      >> Can you make the font bigger, please?
      >> Oh, yeah, of course.  Thank you.  How is that?  Better?
      Okay.  So this is a trace describing what's going on here.  So you can kind of read this a little bit like a chart.  So times going from left to right, and then kind of down the stack as we go.  So we can see each of these different bars here correspond to different operations.  The top one is the browser's kind of view of the world, and then we hit some back agendas.  In the dapper terminology, these are called spans, but it's really just a timed event at some level.  Anyway, the idea here is that we can look and see there's time spent on the client.  But then this pink part here, this is really a problem.  At least in this case we can take a look at some of these other ones and see what's going on.
      Yeah, the fryer, really, a second in the fryer here.  So, you know, right away we've gone from user visible experiences, not great, to we know what component the system we need to be looking at.  And that's great.  So we've made some progress.
      And, you know, this is, like, a tiny little example, like, in the real world, these things can have hundreds or thousands of spans, so being able to automate finding of the critical path is really helpful and really, like, gets you going in terms of finding a solution.
      So, yeah.  So this was great.  But sort of a gap here, which is, like -- and this is where dapper kind of stopped.  And where most other tracing systems stopped.  It would be great to understand why that thing was slow, and it would be great to do that in a general and automated way.  And this is sort of the last part, but I want to do kind of a brief aside on tracing and why it's hard.
      So really, tracing is just logging at some level we call it tracing, but in monitoring in general, there's two things.  There's statistics, and there's events.  And events may include tracing and logging.  But I think when people say tracing or at least distributed tracing, meaning there's some kind of sampling strategy that's a little bit smart, maybe.  And then that they can see some kind of causality or structure not just time stamps.
      So that problem with logs, you know, I had this diagram here for one transaction -- oh, sorry.  You guys can't see this.
      Here we go.  Sorry.  Yeah.  So tracing is -- excuse me one transaction that a lot of logs for that.  Because you don't want to put your developers in a position where they don't want a log.  Like, logging is a valuable thing.  But if you're logging a lot, you know, for every microservice and every transaction, that can get pretty expensive.  Or even just kind of infeasible from a throughput point of view.
      So really, this is the idea of the problem that dapper was solving at Google is that they wanted to do logging for Web search requests.  But even the public numbers for Google are something, like, 2 billion requests a second.  So just no way to log all of that data.  So the solution was basically to throw out 99.9% of it.  It turns out that -- like .01% of 2 billion is still pretty big.  You can still find whatever signals you're looking for in there.
      But you have to do that in a kind of smart way in that you want to make sure that you capture the whole transaction so that it's still useful.
      But if you take the number of transactions, like I said, and do it naïvely, it could be pretty expensive.
      So let's talk for a minute about sampling.  There's really kind of three points that you can do sampling.  One is you can do it in the process to limit the impact on the CPU.  You can also do it in process to, like, for the -- to save network costs.  And then once you've sent it over the network, maybe you buffer it somewhere and do some more sampling to limit the cost of storage.
      In the end, the one that really matters is just the last one.  Like, this is the only reason that we did any sampling at Google.  Only for the most high throughput services did we ever see any impact on the CPU.  And, really, that's really about throughput, not latency.  There's a way to move all of this instrumentation through the impact itself.  And the throughput was a small percent, and that was fine for everyone, except the biggest, biggest parts of Web search.
      So sampling and the process is really not necessary, and I think that's really important to understand because if you're not sampling in the process, you can make much better decisions about what you choose to keep or not keep.
      Of course, you don't want to render all the disks, that's crazy.  And there's some tricks to making this work.  But there's a big gap between tracing systems that do all of this and where they should be, I think.
      So this gets us back to the last part of the story.  Understanding why.
      So I have this diagram before where I was trying to isolate distributed transactions.  It's not really true.  Like, they're not really independent.
      So, you know, yeah, and that the rest of the demo, I think the reason I need your help is to show that.
      So I have this hypothesis that most latency issues are actually due to contention for shared resources.  And that means when I'm looking at a trace, I see there's a slow span.  That request is actually not the problem.  The problem is some other request that's happening at the same time.  And I tried to frame this in a stark way.  Like, maybe not most.  But I think it's probably most -- I think the reason I wanted to do that is there's kind of a blind spot in our tools, and I think that also biases the way we kind of think about the problem.  Like, tools are pretty bad at understanding in other transactions happening at the same time, and I think that we kind of internalize that in a way that makes us think it's not that important.
      So, yeah.  So I had this thing before about independent tracing.  But really, the red pillow reality that we really live with is that these things are all inner dependent.  So okay.
      So, you know, I had this done as a service thing that I'm running.  I need to understand what's going on.  So here's what I did.  I assigned what I call resource identifiers to all of the important resources.  This is really a small wrapper around the library in go.  It's, like, 50 lines.  The actual application changes, I'll show you in a second, but it's one or two lines.
      We then tag all of the spans with that resource ID whenever they try to grab that resource or use it.  And then three is sort of the magic part where we figure out how to interference these things and capture that and if we've done that, then we can answer the why question pretty directly.
      So like I said, the codes, it's really not that interesting.  So this is, like, drastic oversimplification of how you might some code in go.
      Of course, you want to defer that unlock or something like that.  Maybe do something between lock and unlock.  But really, the only change I had to make here was one to wrap the lock itself.  I was explicit about the tracer itself.  That could be done with a single ton, but I just wanted to be clear and when I lock, we can figure out how to associate this with the request.
      It's pretty easy.  Smart lock itself is about 50 lines.  It's really about generating a number, really.
      And that's it; right?  So this is how it's going to kind of go.  So I'll give you a little preview of what it's going to look like and what's going to happen behind the scenes.
      So this is one of these traces; right?  And we've got this long child span, that's the problem; right?  This long one is kind of in the middle here.  And we have a resource ID that is going to be associated with that.  This sort of random number.  And then what we're going to do when we notice this happens, we're going to go and find all of the other spans that are contending for that same resource, and then we're going to go up the stack on all of those things and aggregate the data from those.  So that will give us a big picture of everything that's happening at the time and not just from a low level but high level from the user point of view of what's going on.
      And then we'll use it to figure out what's going on.  And I think the real beauty of this is that the tracing system doesn't really care about the kind of resource or how it works.  Like, really, the assumptions that the tracing system is making a unique ID for this thing, and it can find other spans with that unique ID.  And that's really it.  So you should be able to apply the same thing to a database network conditions, machine resources, whatever it is.  It really doesn't matter.
      So okay.  Let's get back to doughnuts.  This is where I need your help.
      So this is the weird part where a speaker asks you to get out your phone or laboratory, and I want everyone to go to donut.zone, and I'm going to divide everyone in a two groups here.  So everyone on the right-hand side, we're going to call you group A.  Sorry.  Middle thing has to be awkwardly divided.  But I would like you all to order miscellaneous doughnuts.  I'll show you what that looks like in a second.  I want everyone on this side, that's your left to order lots of sprinkle doughnuts.  Like, go crazy.  You haven't ordered any lunch today.  You really need sprinkle doughnuts.
      Let's see what this all looks like here.  How are we doing in terms of -- actually, before, let me just restock here because we're a little bit low.
      Cool.
      Okay.  So --
      Okay.  Is everybody ready?  If you're not doing it, do it right now.  You guys miscellaneous doughnuts, everybody over here, lots of sprinkles.  Three, two, one, go.
      And let's see how we're doing here.  All right.  That's probably good.  Thanks.  You can take a break.  Not too many doughnuts.  Let's see what's going on here.
      Yeah, lots of slow things happening.  Let's take a look at what's going on here.
      Okay.  Cool.  11 seconds.  That's not good.  That's not a good user experience.  So let's see here.  So this was actually a cinnamon doughnut in the end, not even a sprinkles one, but it's really slow.  And it turns out in the example I did before that the fryer is really the contention here.  What we can do then is drill down on that and take a look at what's going on over here.  Try to zoom in.  This is a really janky graph library that we hacked into this thing.
      Sorry.  But okay.  So here's a thing, like, this is basically showing kind of a aggregate graph going back up.  So fried doughnuts at the bottom here, that's the thing that's grabbing the resource.  That all gets called for make doughnut, gets called from a bunch of places, whether it's cinnamon or chocolate or whatever.
      But what we're seeing here is that even though this is a cinnamon doughnut that it's contending for a lock that 84 -- or sorry.  80% -- 79% of the load on that lock actually comes from sprinkles; right?  That's this half of the room.  And the thing that's kind of cool about this is that we didn't say anything about tags or cardinality or any of this.  But we identified some important information for some of these things, but the fact that, you know, there could have been 100 different, 200, 1,000 different values for this thing, it wouldn't really matter.  We're just doing this stuff sort of in realtime and aggregating just sort of based off of what was important here.
      Cool.  Ready?  Okay.  So let's go on to number two.  Let me remember to flip this back.
      Okay.  So group A, I want you all to keep doing the same thing.  Just casually keep -- not right now.  You can -- don't eat too many doughnuts.  When I say go, keep doing the thing you were doing.  But group B, I want you to go down to the bottom.  There's this restock button here, and I want you to click on restock, and then I want you to restock a bunch of all of the things accelerate does that make sense?  Okay.
      So everybody ready?  I think we can do that now.  Ready?  Three, two, one, go.  Order random doughnuts.  Restock lots of stuff.
      Okay.  That's probably good.  You can take a break, like I said.  You've been working hard eating doughnuts.
      Okay.  So let's see what's going on here.  Okay.  Things are still pretty slow.  Let's grab a couple of these and see what's going on.  So these are, again, we're looking at making doughnuts here.  And, yeah.  A little bit of network issues here, something like that.  This one's still fire.  Let's take a look at this one.
      Yeah, fire's still pretty bad on this one.  But if I look at the toppings here, let's just imagine for a second that the fire was actually not the problem in this case, but it was the toppings.  In this case, what we actually have is not just one call graph but two because there's not any shared code graph between these things.  It's just a few resources; right?  And I've seen this come up a bunch of times.  You can be all two different teams at a company.  Not really realizing that you're working on the same shared resource.  But even though I'm looking at something that's about making doughnuts, 65% of the load -- sorry.  It's super sensitive.  Not doing it for effect.  65% of the load on this thing comes from the restocking team; right?
      So this is the kind of thing that's meant to show is, you know, where you might have, you know, a new release of the service that's coming out over here, puts a lot of load on the resource, now this is really slow and you don't understand.  Nothing on your side changed.  And this allows you to go right to that thing and find that answer and be able to go to the other team and say, hey, what's going on?  Like, you have to stop doing this or we have to provision this resource differently.
      All right.  So I have one last demo.  I need kind of a volunteer for this one.  Does anyone -- one person want to volunteer?
      Okay.  I would like you to clean the fryer, the very generous volunteer.  So at the bottom of the thing, there's a button for clean the fryer.  I want everyone else to do what you were doing before.  So you guys are organized doughnuts.  You guys are restocking my one new friend is cleaning the fryer.  Everybody is doing that right now.  Just keep cleaning the fryer too.  It's pretty dirty.  There's a lot of doughnuts.
      Okay.  That's good.  Thanks.  All right.
      So let's see what's going on here.  Yeah, not looking good.  Pretty slow.  Got some 11-second ones here.  Ten seconds.  20 seconds.  Not good.  Got a lot of queuing going on here.  A lot of work to do once I get back home.
      Okay.  So let's look at some of these things when I see what's going on here.
      So not surprisingly, like, 20 seconds, waiting on that fryer.  And the thing that's kind of crazy here is, like, this, if I can make it bigger, yeah, okay.  Sorry.  It's hard to talk and scroll at the same time.  So this is one person responsible for 88% of the load on this resource, and this is actually another thing that can happen as well.  You can have one bad piece of code, you can have one bad customer.  And doing kind of a generic, like, look at the whole world and just P99 or something like that.  P99 might show all of this problem, but you're never going to see that one, small user and one, small service or something like that that's causing the problem.
      But if you have all of the data, you can do this kind of thing.  So, again, I think it's kind of really cool and exciting thing that we should be able to do with the tracing systems we have.
      My friend and I totally hacked this up, so none of this works, it's a janky demo that we made.  But the data is there, there's no reason we couldn't build systems like this.
        I just wanted to kind of scroll to the right screen and wrap up here.
      So a couple of things I would like you to take away.  Think hard.  When you have latency problems about resource contention.  I think it's more common than we might believe.  I think that there's an opportunity to do really game-changing tooling that can enable this kind of realtime contention analysis.
      And, you know, if we can do that, we can really cut down the time to remediate these problems to root cause these bottle next so that we can do that super, super fast by using tools to help us do that; right?  I think this is the kind of thing where we should be developing systems that help us be smarter, that help us work better.  And I think we really, really can do that.
      So the very last thing, there's a couple of links on here if you're excited about tracing, I encourage you to visit open tracing.io.  It's an open standard for getting tracing data out of applications.  All of the code that I wrote for this doughnut thing is all just using open tracing.  And then I like talking about this stuff.  If you like talking about this stuff, I'm going to go to the bridge tap house after Adam's talk later tonight.  If you want to meet up there and grab something to eat, hit me up on Slack, and we can try to coordinate, and it will be fun.  Thanks.
      [Applause]
      Captions provided by @chaselfrazier and @whitecoatcapxg.
